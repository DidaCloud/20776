Module 2: Processing Event Streams using Azure Stream Analytics

Lab: Process event streams with Stream Analytics
---------------------------------------------

### Logon Information
Virtual machine: **20776A-LON-DEV**
Username: **ADATUM\AdatumAdmin**
Password: **Pa55w.rd**

### Estimated Time
120 minutes

### Scenario
You work for Adatum as a data engineer, and have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data that holds vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For the first phase of the project, you will use Stream Analytics, together with Event Hubs, IoT Hubs, Service Bus, and custom applications to:
- Provide insights into average speeds at various locations.
- Determine the locations of police patrol cars.
- Present vehicle locations on a map.
- Check vehicles recorded by speed cameras against a list of stolen vehicles.
- Determine the nearest patrol car to a speeding vehicle or stolen vehicle, send a dispatch alert to the nearest patrol car, and show the dispatched patrol car locations on a map.
- Use Stream Analytics monitoring and alerting tools to help identify issues during the system's deployment, and use the Azure portal and PowerShell to scale up and scale down the system to cope with particular demands.

### Objectives
After completing this lab, you will be able to:
- Create a Stream Analytics job to process event hub data.
- Create a Stream Analytics job to process IoT hub data.
- Reconfigure a Stream Analytics job to send output through a Service Bus queue.
- Reconfigure a Stream Analytics job to process both event hub and static file data.
- Use multiple Stream Analytics jobs to process event hub, IoT hub and static file data, and output results using a Service Bus and custom application.
- Use the Azure portal and PowerShell to manage and scale Stream Analytics jobs.

### Instructor Note?
In Exercise 1, students require credentials for the Power BI service. You cannot use outlook.com credentials with Power BI; you need an “enterprise” email address. Therefore, students might already have credentials that will work with Power BI. If students do not have a Power BI login, direct them to: [https://Power BI.microsoft.com/en-us/documentation/Power BI-admin-signing-up-for-power-bi-with-a-new-office-365-trial][42e35dc9]. Students then follow the steps to create an account. This option gives students a sign-in of the following form: user@something.onmicrosoft.com.

  [42e35dc9]: https://Power BI.microsoft.com/en-us/documentation/Power BI-admin-signing-up-for-power-bi-with-a-new-office-365-trial "Signing up for Power BI with a new Office 365 trial"



## Exercise 1: Create a Stream Analytics job to process event hub data

### Scenario
For the first phase of the project, you will start to build the traffic surveillance system to provide insights into average speeds at various locations. In this exercise, you will create a Stream Analytics job that captures speed camera data sent to an event hub from a Visual Studio application (SpeedCameraDevice). You will configure the Stream Analytics job to send output data to a Power BI dashboard and to Azure Data Lake Storage, using filters to remove unnecessary fields before storage.

### Result
At the end of this exercise, you will have created an Azure Data Lake Store, an event hubs namespace, and a Stream Analytics job. You will then use Stream Analytics to process event hubs data, and view the results in a Power BI dashboard and in Data Lake Store.

### Instructor Note
At the time of writing, Data Lake Store is only available in three regions. Students should choose their nearest location in the first task, and then (to minimize potential cross-region latency issues) use the same location for all other Azure resources in this lab.

### Task 1: Create a Data Lake Store
1.	Ensure that the MT17B-WS2016-NAT, 20776A-LON-DC, and 20776A-LON-DEV virtual machines are running, and then log on to 20776A-LON-DEV as ADATUM\AdatumAdmin with the password Pa55w.rd.
2.	On the Start menu, type Internet Explorer, and then press Enter.
3.	In Internet Explorer, go to http://portal.azure.com, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
4.	In the Azure portal, click + New, click Storage, and then click Data Lake Store.
5.	On the Data Lake Store blade, click Create.
6.	On the New Data Lake Store blade, in the Name box, type adls<your name><date>.
7.	Under Resource Group, click Create new, and type CamerasRG.
8.	In the Location list, select your nearest location from the currently available Data Lake Store regions.
9.	Leave all other settings at their defaults, and click Create.
10.	Wait until the storage has deployed before continuing with the lab.
