Module 2: Processing Event Streams using Azure Stream Analytics

Lab: Process event streams with Stream Analytics
---------------------------------------------

### Logon Information
Virtual machine: **20776A-LON-DEV**  
Username: **ADATUM\AdatumAdmin**  
Password: **Pa55w.rd**

### Estimated Time
120 minutes

### Scenario
You work for Adatum as a data engineer, and have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data that holds vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For the first phase of the project, you will use Stream Analytics, together with Event Hubs, IoT Hubs, Service Bus, and custom applications to:
- Provide insights into average speeds at various locations.
- Determine the locations of police patrol cars.
- Present vehicle locations on a map.
- Check vehicles recorded by speed cameras against a list of stolen vehicles.
- Determine the nearest patrol car to a speeding vehicle or stolen vehicle, send a dispatch alert to the nearest patrol car, and show the dispatched patrol car locations on a map.
- Use Stream Analytics monitoring and alerting tools to help identify issues during the system's deployment, and use the Azure portal and PowerShell to scale up and scale down the system to cope with particular demands.

### Objectives
After completing this lab, you will be able to:
- Create a Stream Analytics job to process event hub data.
- Create a Stream Analytics job to process IoT hub data.
- Reconfigure a Stream Analytics job to send output through a Service Bus queue.
- Reconfigure a Stream Analytics job to process both event hub and static file data.
- Use multiple Stream Analytics jobs to process event hub, IoT hub and static file data, and output results using a Service Bus and custom application.
- Use the Azure portal and PowerShell to manage and scale Stream Analytics jobs.

### Instructor Note?
In Exercise 1, students require credentials for the Power BI service. You cannot use outlook.com credentials with Power BI; you need an “enterprise” email address. Therefore, students might already have credentials that will work with Power BI. If students do not have a Power BI login, direct them to: [https://Power BI.microsoft.com/en-us/documentation/Power BI-admin-signing-up-for-power-bi-with-a-new-office-365-trial][42e35dc9]. Students then follow the steps to create an account. This option gives students a sign-in of the following form: user@something.onmicrosoft.com.

  [42e35dc9]: https://Power BI.microsoft.com/en-us/documentation/Power BI-admin-signing-up-for-power-bi-with-a-new-office-365-trial "Signing up for Power BI with a new Office 365 trial"



## Exercise 1: Create a Stream Analytics job to process event hub data

### Scenario
For the first phase of the project, you will start to build the traffic surveillance system to provide insights into average speeds at various locations. In this exercise, you will create a Stream Analytics job that captures speed camera data sent to an event hub from a Visual Studio application (SpeedCameraDevice). You will configure the Stream Analytics job to send output data to a Power BI dashboard and to Azure Data Lake Storage, using filters to remove unnecessary fields before storage.

### Result
At the end of this exercise, you will have created an Azure Data Lake Store, an event hubs namespace, and a Stream Analytics job. You will then use Stream Analytics to process event hubs data, and view the results in a Power BI dashboard and in Data Lake Store.

### Instructor Note
At the time of writing, Data Lake Store is only available in three regions. Students should choose their nearest location in the first task, and then (to minimize potential cross-region latency issues) use the same location for all other Azure resources in this lab.

### Task 1: Create a Data Lake Store
1.	Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\AdatumAdmin** with the password **Pa55w.rd**.
2.	On the Start menu, type **Internet Explorer**, and then press Enter.
3.	In Internet Explorer, go to http://portal.azure.com, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
4.  In the Azure portal, click **+ New**, click **Storage**, and then click **Data Lake Store**.
5.  On the **Data Lake Store** blade, click **Create**.
6.  On the **New Data Lake Store** blade, in the **Name** box, type **adls*\<your name\>\<date\>***.
7.  Under **Resource** **Group**, click **Create new**, and type **CamerasRG**.
8.  In the **Location** list, select your nearest location from the currently available Data Lake Store regions.
9.  Leave all other settings at their defaults, and click **Create**.
10.	Wait until the storage has deployed before continuing with the lab.

### Task 2: Create an event hubs namespace and hub
1.  In the Azure portal, click **+ New**, click **Internet of Things**, and then click **Event Hubs**.
2.  On the **Create namespace** blade, in the **Name** box, type **camerafeeds*\<your name\>\<date\>***.
3.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
4.  In the **Location** list, select the same location as you used for the Data Lake Store.
5.  Leave all other settings at their defaults, and click **Create**.
6.  Wait until the namespace has deployed before continuing with the lab.
7.  Click **All resources**, click **camerafeeds*\<your name\>\<date\>***, and then click **+ Event Hub**.
8.  On the **Create Event Hub** blade, in the **Name** box, type **traffic**.
9.  Set the **Partition Count** to **16**.
10. Leave all other settings at their defaults, and click **Create**.
11. Wait until the event hub has deployed before continuing with the lab.
12. On the **camerafeeds*\<your name\>\<date\>*** blade, under **ENTITIES**, click **Event Hubs**, and then click **traffic**.
13. On the **traffic** blade, click **+ Consumer Group**.
14. On the **Create consumer group** blade, in the **Name** box, type **cameradatafeed**, and then click **Create**.
15. On the **traffic** blade, click **+ Consumer Group**.
16. On the **Create consumer group** blade, in the **Name** box, type **cameradatafeed2**, and then click **Create**.
17. Close the traffic blade.
18. On the **camerafeeds*\<your name\>\<date\>* - Event Hubs** blade, under **SETTINGS**, click **Shared access policies**, and then click **RootManageSharedAccessKey**.
19. Next to the **PRIMARY KEY**, click the **Click to copy** button, to copy the primary key to the clipboard.
20. On the Start menu, type **Notepad**, and then press Enter.
21. In Notepad, type **Event hub primary key**, and press Enter.
22. On the **Edit** menu, click **Paste**, to store the primary key.
23. On the **File** menu, click **Save**.
24. In the **Save As** dialog box, go to **E:\\Labfiles\\Lab02**, in the **File name** box, type **Config\_details.txt**, and then click **Save**.

### Task 3: Create a Stream Analytics job
1.  Switch to the Azure portal, click **+ New**, click **Data + Analytics**, and then click **Stream Analytics** **job**.
2.  On the **New Stream Analytics** **Job** blade, in the **Job name** box, type **TrafficAnalytics**.
3.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
4.  In the **Location** list, select the same location as you used for the Data Lake Store, and then click **Create**.
5.  Wait until the Stream Analytics job has deployed before continuing with the lab.

### Task 4: Configure Stream Analytics job inputs
1.  In the Azure portal, click **All resources**, and then click **TrafficAnalytics**.
2.  On the **TrafficAnalytics** blade, under **JOB TOPOLOGY**, click **Inputs**, and then click **+ Add**.
3.  On the **New input** blade, enter the following details:

Property  |  Value
--|--
**Input alias**   |  CameraDataFeed
**Source Type**  |  Data stream
**Input alias** | CameraDataFeed
**Source Type** | Data stream
**Source** | Event hub
**Import option** | Provide event hub settings manually
**Service bus namespace** | camerafeeds*\<your name\>\<date\>* as you created earlier
**Event hub name** | traffic
**Event hub policy name** | RootManageSharedAccessKey
**Event hub policy key** | Paste the key you copied into Config\_details.txt
**Event hub consumer group** | cameradatafeed
4. Leave all other settings at their defaults, and click **Create**.
5. Wait until the input has been successfully created before continuing with the lab.
6. On the **TrafficAnalytics - Inputs** blade, click **+ Add**.
7.  On the **New input** blade, enter the following details:
    -   **Input alias**: CameraDataFeed2
    -   **Source Type**: Data stream
    -   **Source**: Event hub
    -   **Import option**: Provide event hub settings manually
    -   **Service bus namespace**: camerafeeds*\<your name\>\<date\>* as you created earlier
    -   **Event hub name**: traffic
    -   **Event hub policy name**: RootManageSharedAccessKey
    -   **Event hub policy key**: Paste the key you copied to Config\_details.txt
    -   **Event hub consumer group**: cameradatafeed2
8. Leave all other settings at their defaults, and click **Create**.
9. Wait until the input has been successfully created before continuing with the lab.
